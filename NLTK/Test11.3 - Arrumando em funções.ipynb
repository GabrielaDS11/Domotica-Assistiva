{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "STOPWORDS = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(dados):\n",
    "    df = pd.read_csv(dados, sep=';')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(text):\n",
    "    #transformar em letras minusculas\n",
    "    text = text.lower()\n",
    "    #retirar acentos\n",
    "    #retirar characters especiais\n",
    "    text = re.sub(r'\\W',' ',text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+',' ',text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(sentence):\n",
    "    stemmer = SnowballStemmer(language='portuguese')\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatization(sentence):    \n",
    "    for word in sentence:\n",
    "        print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(texto_df, ngrams):\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    vectorizer = CountVectorizer(min_df = 2, lowercase=True,ngram_range =(1,ngrams),stop_words = STOPWORDS, tokenizer = token.tokenize)\n",
    "    text_counts = vectorizer.fit_transform(texto_df)\n",
    "    \n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X):\n",
    "    \n",
    "    processed_features = X\n",
    "    vectorizer = TfidfVectorizer (max_features=500, min_df=2, max_df=0.8, stop_words=STOPWORDS)\n",
    "    processed_features = vectorizer.fit_transform(processed_features).toarray()\n",
    "    \n",
    "    return processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X,y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "    matrix_X = X_train\n",
    "    \n",
    "    X_train = X_train.toarray()\n",
    "    X_test = X_test.toarray()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, matrix_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing2(X,y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "    matrix_X = X_train\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, matrix_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(X_train, X_test, y_train, y_test):\n",
    "    forest = RandomForestClassifier(max_leaf_nodes=3, random_state=0)\n",
    "    forest.fit(X_train, y_train)\n",
    "    y_pred = forest.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return score, report, forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(X_train, X_test, y_train, y_test):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return score, report, gnb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(X_train, X_test, y_train, y_test,input_valor):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=50, activation='relu',input_dim = input_valor))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=10, activation='relu'))\n",
    "    model.add(Dense(units=5, activation='relu'))\n",
    "    \n",
    "    summary=model.summary()\n",
    "    compiled = model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "    fitted = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=7, batch_size=10)\n",
    "    \n",
    "    return summary, compiled, fitted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = loadData(\"tweet19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.columns = ['assunto','texto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['texto'] = dataframe['texto'].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assunto</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abertura das praias</td>\n",
       "      <td>mudou absolutamente nada paciência papo novo n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abertura das praias</td>\n",
       "      <td>opinião mudou nada pessoas agindo vírus sido d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abertura das praias</td>\n",
       "      <td>uol incentivando pessoas irem praia plena pand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abertura das praias</td>\n",
       "      <td>ônibus trens lotados tbm desde sempre praia po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abertura das praias</td>\n",
       "      <td>novo normal sei onde engraçado povo posando fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               assunto                                              texto\n",
       "0  abertura das praias  mudou absolutamente nada paciência papo novo n...\n",
       "1  abertura das praias  opinião mudou nada pessoas agindo vírus sido d...\n",
       "2  abertura das praias  uol incentivando pessoas irem praia plena pand...\n",
       "3  abertura das praias  ônibus trens lotados tbm desde sempre praia po...\n",
       "4  abertura das praias  novo normal sei onde engraçado povo posando fo..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uol incentivando pessoas irem praia plena pandemia sim plena pandemia isolamento social necessário caso contrário sairemos nunca dessa situação fiquememcasa'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['texto'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'at', 'est', 'estiv', 'f', 'h', 'houv', 'j', 'm', 'n', 'nhamos', 'ramos', 's', 'ser', 'ssemos', 't', 'tamb', 'ter', 'tiv', 'vamos', 'voc'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "text_counts = bag_of_words(dataframe['texto'],2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = dataframe.iloc[:,1], dataframe.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abertura das praias', 'profissionais', 'transporte público',\n",
       "       'vacina', 'volta as aulas'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38x133 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 335 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, matrix_train = preprocessing(text_counts,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30x133 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 249 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 133)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracia Naive Bayes: \n",
      "87.5 %\n",
      "Report Naive Bayes: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.88         8\n",
      "   macro avg       0.75      0.62      0.67         8\n",
      "weighted avg       1.00      0.88      0.92         8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gnb_score, gnb_report, gnb  = gaussian(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracia Naive Bayes: \")\n",
    "print(gnb_score,\"%\")\n",
    "print(\"Report Naive Bayes: \")\n",
    "print(gnb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracia Random Forest: \n",
      "25.0 %\n",
      "Report Random Forest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       1.00      0.33      0.50         3\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.25         8\n",
      "   macro avg       0.50      0.21      0.29         8\n",
      "weighted avg       0.62      0.25      0.35         8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest_score, forest_report, forest  = randomForest(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracia Random Forest: \")\n",
    "print(forest_score,\"%\")\n",
    "print(\"Report Random Forest: \")\n",
    "print(forest_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                6700      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 8,290\n",
      "Trainable params: 8,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 30 samples, validate on 8 samples\n",
      "Epoch 1/7\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 5.9089 - accuracy: 0.1333 - val_loss: 4.7754 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/7\n",
      "30/30 [==============================] - 0s 432us/step - loss: 5.3609 - accuracy: 0.2333 - val_loss: 4.6939 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/7\n",
      "30/30 [==============================] - 0s 366us/step - loss: 3.6966 - accuracy: 0.3000 - val_loss: 4.9997 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/7\n",
      "30/30 [==============================] - 0s 382us/step - loss: 3.1255 - accuracy: 0.3667 - val_loss: 7.5096 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/7\n",
      "30/30 [==============================] - 0s 432us/step - loss: 2.6600 - accuracy: 0.3667 - val_loss: 7.3579 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/7\n",
      "30/30 [==============================] - 0s 499us/step - loss: 2.5166 - accuracy: 0.5000 - val_loss: 7.0941 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/7\n",
      "30/30 [==============================] - 0s 465us/step - loss: 2.4346 - accuracy: 0.5333 - val_loss: 6.1175 - val_accuracy: 0.0000e+00\n",
      "Sumário Rede Neural:\n",
      "None %\n",
      "Rede Neural....\n",
      "<keras.callbacks.callbacks.History object at 0x000001ED5BBACD48>\n"
     ]
    }
   ],
   "source": [
    "summary, compiled, fitted = neural(X_train, X_test, y_train, y_test, X_train.shape[1])\n",
    "print(\"Sumário Rede Neural:\")\n",
    "print(summary,\"%\")\n",
    "print(\"Rede Neural....\")\n",
    "print(fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.33150395, 0.36060372,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_X = tfidf(X)\n",
    "processed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, matrix_train = preprocessing2(processed_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    processed_X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracia Naive Bayes: \n",
      "75.0 %\n",
      "Report Naive Bayes: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       0.67      1.00      0.80         2\n",
      "           4       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.57      0.76      0.63        12\n",
      "weighted avg       0.68      0.75      0.69        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gnb_score, gnb_report, gnb  = gaussian(X_train2, X_test2, y_train2, y_test2)\n",
    "print(\"Accuracia Naive Bayes: \")\n",
    "print(gnb_score,\"%\")\n",
    "print(\"Report Naive Bayes: \")\n",
    "print(gnb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracia Random Forest: \n",
      "25.0 %\n",
      "Report Random Forest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.18      1.00      0.31         2\n",
      "           4       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.25        12\n",
      "   macro avg       0.24      0.40      0.26        12\n",
      "weighted avg       0.11      0.25      0.13        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest_score, forest_report, forest  = randomForest(X_train2, X_test2, y_train2, y_test2)\n",
    "print(\"Accuracia Random Forest: \")\n",
    "print(forest_score,\"%\")\n",
    "print(\"Report Random Forest: \")\n",
    "print(forest_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 50)                5100      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 6,690\n",
      "Trainable params: 6,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26 samples, validate on 12 samples\n",
      "Epoch 1/7\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 4.0818 - accuracy: 0.3462 - val_loss: 3.3110 - val_accuracy: 0.3333\n",
      "Epoch 2/7\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 4.0746 - accuracy: 0.3846 - val_loss: 4.0533 - val_accuracy: 0.3333\n",
      "Epoch 3/7\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.7897 - accuracy: 0.4231 - val_loss: 3.9914 - val_accuracy: 0.3333\n",
      "Epoch 4/7\n",
      "26/26 [==============================] - 0s 690us/step - loss: 2.9286 - accuracy: 0.4231 - val_loss: 3.9258 - val_accuracy: 0.3333\n",
      "Epoch 5/7\n",
      "26/26 [==============================] - 0s 689us/step - loss: 1.4358 - accuracy: 0.4615 - val_loss: 3.8590 - val_accuracy: 0.3333\n",
      "Epoch 6/7\n",
      "26/26 [==============================] - 0s 691us/step - loss: 1.2025 - accuracy: 0.5000 - val_loss: 3.7959 - val_accuracy: 0.3333\n",
      "Epoch 7/7\n",
      "26/26 [==============================] - 0s 690us/step - loss: 1.3088 - accuracy: 0.5385 - val_loss: 3.6640 - val_accuracy: 0.3333\n",
      "Sumário Rede Neural:\n",
      "None %\n",
      "Rede Neural....\n",
      "<keras.callbacks.callbacks.History object at 0x000001ED5D2328C8>\n"
     ]
    }
   ],
   "source": [
    "summary, compiled, fitted = neural(X_train2, X_test2, y_train2, y_test2, X_train2.shape[1])\n",
    "print(\"Sumário Rede Neural:\")\n",
    "print(summary,\"%\")\n",
    "print(\"Rede Neural....\")\n",
    "print(fitted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
